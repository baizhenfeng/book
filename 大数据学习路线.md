@[Toc](大数据路线)

# 大数据学习路线

> 大数据的本质：两个
>
> 1. 大数据的存储——分布式文件存储
> 2. 大数据的计算——分布式计算
>
> 大数据核心框架：两个
>
> 1. Hadoop——基于Java语言开发
> 2. Spark——基于Scala语言开发，Scala语言基于Java语言。Spark支持Java语言，但使用Scala语言更优。

## 1.Java基础——主要部分是JavaSE

### 1.1  Java初级

> 1. 基本语法
> 2. 类
> 3. 封装
> 4. 继承
> 5. 多态

```
注：
1.主要指的是JavaSE部分。对于JavaEE(企业级应用，例Tomcat的使用)部分和JavaME(主要应用方向是嵌入式领域，目前大有被Android所替代的趋势)部分，不做掌握要求。JavaEE和JavaME两部分的基础也是JavaSE。
2.Java中没有函数的概念，C/C++中的函数在Java中成为方法，方法中的参数也称为形参，在方法的定义中，是不能使用其他方法作为形参的，但是在方法的调用中，是可以使用方法作为参数的，前提是该方法的返回值必须与形参所定义的值类型一致。
```

***

### 1.2  Java高级

>1. Java多线程基本知识
>2. Java同步关键词详解
>3. java并发包线程池及在开源软件中的应用
>4. Java并发包消息队里及在开源软件中的应用
>5. Java JMS技术
>6. Java动态代理反射
>7. I/O流
>8. 泛型

### 1.3  其余常见基础......

## 2.Linux基础——主要指的是Linux基本命令操作

1.  Linux的介绍，Linux的安装：VMware Workstation虚拟软件安装过程、CentOS虚拟机安装过程
2. Linux的常用命令：常用命令的介绍、常用命令的使用和练习：包括文件/目录常见操作、用户管理与权限、免密登陆配置与网络管理。
3. Linux系统进程管理基本原理及相关管理工具如ps、pkill、top、htop等的使用；
4. Linux启动流程，运行级别详解，chkconfig详解
5. VI、VIM编辑器：VI、VIM编辑器的介绍、VI、VIM扥使用和常用快捷键
6. Linux磁盘管理，lvm逻辑卷，nfs详解
7. Linux系统文件权限管理：文件权限介绍、文件权限的操作
8. Linux的RPM软件包管理：RPM包的介绍、RPM安装、卸载等操作
9. yum命令，yum源搭建
10. Linux网络：Linux网络的介绍、Linux网络的配置和维护 防火墙配置
11. Shell编程：Shell的介绍、Shell脚本的编写
12. Linux上常见软件的安装：安装JDK、安装Tomcat、安装mysql,web项目部署
13. linux高级文本处理命令cut、sed、awklinux
14. 定时任务crontab
15. 其余常见操作......

注：目录，在Linux系统中常称为目录，在Windows系统中常称为文件夹，不同称谓同样的性质。

## 3.Hadoop生态学习（体系结构、原理、编程）

### 3.1第一阶段

> 这一阶段是本节的核心，即HDFS(大数据存储)、MapReduce(大数据计算)、HBse(NoSQL数据库)。
>
> Hadoop是一个对海量数据进行处理的分布式系统架构，可以理解为Hadoop就是一个对大量的数据进行分析的工具，和其他组件搭配使用，来完成对大量数据的收集、存储和计算。
>
> 有一个基于Hadoop的数据挖掘库——Mahout。

``` 
离线计算系统Hadoop体系基础主要内容：
1、Hadoop快速入门
    hadoop背景介绍
    分布式系统概述
    离线数据分析流程介绍
    集群搭建
    集群使用初步

2、HDFS
    HDFS的概念和特性
    HDFS的shell(命令行客户端)操作
    HDFS的工作机制
    NAMENODE的工作机制
    java的api操作
    案例1：开发shell采集脚本

3、MapReduce基础
    自定义Hadoop的RPC框架
    Mapreduce编程规范及示例编写
    Mapreduce程序运行模式及debug方法
    MapReduce程序运行模式的内在机理
    MapReduce运算框架的主体工作流程
    自定义对象的序列化方法
    MapReduce编程案例

4、MapReduce高级
    Mapreduce排序
    自定义partitioner
    Mapreduce的combiner
    mapreduce工作机制详解

5、MapReduce实战
    maptask并行度机制-文件切片
    maptask并行度设置
    倒排索引
    共同好友
    
Hadoop入门——>HDFS/MapReduce基础——>HDFS/MapReduce高级——>HDFS/MapReduce实战
```

### 3.2第二阶段

> 数据分析引擎——Hive(数据仓库工具，不是数据库工具。数据仓库是逻辑上的概念，底层使用的是数据库。)、Pig(Pig是一个基于Hadoop的大规模数据分析平台，它提供的SQL-LIKE语言叫Pig Latin)
>
> 数据采集引擎——Flume(实时日志采集)、Sqoop(数据迁移工具，主要用于在Hive数据库与关系型数据库间进行数据的传递，可将关系型数据库中的数据导入Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。)、DataX(阿里开源)
>
> 注：关系型数据库包括，MySQL、Oracle、DB2、Microsoft SQL Server、Microsoft Access、PostgreSQL等。

```
数据仓库工具Hive基础主要内容：
    1) Hadoop的HA机制
    2) HA集群的安装部署
    3) 集群运维测试之Datanode动态上下线
    4) 集群运维测试之Namenode状态切换管理
    5) 集群运维测试之数据块的balance
    6) HA下HDFS-API变化
    7) hive简介
    8) hive架构
    9) hive安装部署
    10) hvie初使用

Hive高级
    1) HQL-DDL基本语法
    2) HQL-DML基本语法
    3) HIVE的join
    4) HIVE 参数配置
    5) HIVE 自定义函数和Transform
    6) HIVE 执行HQL的实例分析
    7) HIVE最佳实践注意点
    8) HIVE优化策略
    9) HIVE实战案例
```



```
分布式日志框架Flume主要内容：
    1) flume简介/介绍-基础知识
    2) flume安装部署与测试
    3) flume部署方式
    4) flume source相关配置及测试
    5) flume sink相关配置及测试
    6) flume selector 相关配置与案例分析
    7) flume Sink Processors相关配置和案例分析
    8) flume Interceptors相关配置和案例分析
    9) flume AVRO Client开发
    10) flume 和kafka 的整合
    11) 案例：采集目录到HDFS
    12) 案例：采集文件到HDFS
```

```
数据迁移工具Sqoop主要内容：
    1) 介绍 和 配置Sqoop
    2) Sqoop shell使用
    3) Sqoop-import   a) DBMS-hdfs   b) DBMS-hive   c) DBMS-hbase
    4) Sqoop-export
```

### 3.3第三阶段

> 实现Hadoop的HA(HA是High availability的缩写，即高可用，7*24小时不中断服务)——ZooKeeper(Zookeeper是分布式协调管理服务框架，管理分布式环境中的数据。简要来说，Zookeeper = 文件系统 + 监听通知机制。)

```
分布式协调管理服务ZooKeeper主要内容：
     1) ZooKeeper简介及应用场景
     2) ZooKeeper集群安装部署
     3) ZooKeeper的数据节点与命令行操作
     4) ZooKeeper的java客户端基本操作及事件监听
     5) ZooKeeper核心机制及数据节点
     6) ZooKeeper应用案例–分布式共享资源锁
     7) ZooKeeper应用案例–服务器上下线动态感知
     8) ZooKeeper的数据一致性原理及leader选举机制
```



> Web管理工具——Hue(Hue是大数据分析交互平台，是大数据web管理器，是运营和开发Hadoop应用的图形化用户界面。其包括三个主要部分:Hue UI，Hue Server，Hue DB。Hue程序被整合到一个类似桌面的环境，以web程序的形式发布，对于单独的用户来说不需要额外的安装。)
>
> 工作流引擎——Oozie(Oozie，能够提供对Hadoop的MapReduce和Pig的Jobs任务调度与协调。功能相似的任务调度框架还有Azkaban和Zeus。)
>
> 注1：Hadoop-HA严格来说应该分成各个组件的HA机制——*HDFS*的*HA*、*YARN*的*HA*
>
> 注2：大数据四大协作框架——Oozie(任务调度框架)、Sqoop(数据转换工具)、Flume(文件收集库框架)、Hue(大数据Web工具)。

## 4.Spark生态学习

```
内存计算Spark基础主要内容：
    1) Spark介绍
    2) Spark应用场景
    3) Spark和Hadoop MR、Storm的比较和优势
    4) RDD
    5) Transformation
    6) Action
    7) Spark计算PageRank
    8) Lineage
    9) Spark模型简介
    10) Spark缓存策略和容错处理
    11) 宽依赖与窄依赖
    12) Spark配置讲解
    13) Spark集群搭建
    14) 集群搭建常见问题解决
    15) Spark原理核心组件和常用RDD
    16) 数据本地性
    17) 任务调度
    18) DAGScheduler
    19) TaskScheduler
    20) Spark源码解读
    21) 性能调优
    22) Spark和Hadoop2.x整合：Spark on Yarn原理
```

### 4.1第一阶段

> Scala编程
>
> 1.Scala是一门多范式(Multi-paradigm)的编程语言，类似Java编程语言，设计初衷是实现可伸缩的语言、并要集成面向对象编程和命令式编程、函数式编程的各种特性。
>
> 2.Scala是运行在Java虚拟机上的，并兼容现有Java程序。Scala 源代码被编译成Java字节码，所以它可以运行于JVM之上，并可以调用现有的Java类库。
>
> 3.可应用于后端开发，表达能力较强，擅长处理数据，长期运行且吞吐量较大的场景。

```
编程语言Scala主要内容：
    1) scala解释器、变量、常用数据类型等
    2) scala的条件表达式、输入输出、循环等控制结构
    3) scala的函数、默认参数、变长参数等
    4) scala的数组、变长数组、多维数组等
    5) scala的映射、元组等操作
    6) scala的类，包括bean属性、辅助构造器、主构造器等
    7) scala的对象、单例对象、伴生对象、扩展类、apply方法等
    8) scala的包、引入、继承等概念
    9) scala的特质
    10) scala的操作符
    11) scala的高阶函数
    12) scala的集合
    13) scala数据库连接
```

### 4.2第二阶段

> Spark Core——基于内存的数据计算，替代的是Hadoop中的MapReduce部分。MapReduce是基于外存的计算，其计算速度较慢、时效性较差。

### 4.3第三阶段

> Spark SQL——类似于Oracle的SQL语句

### 4.4第四个阶段

> Spark Streaming——进行实时计算（流式计算），典型流式计算的生活场景是自来水厂。

```
流式计算Spark Streaming主要内容：
     1) Spark-Streaming简介
     2) Spark-Streaming编程
     3) 实战：StageFulWordCount
     4) Flume结合Spark Streaming
     5) Kafka结合Spark Streaming
     6) 窗口函数
     7) ELK技术栈介绍
     8) ElasticSearch安装和使用
     9) Storm架构分析
     10) Storm编程模型、Tuple源码、并发度分析
     11) Storm WordCount案例及常用Api分析
```



## 5.Storm学习——实时计算

> Storm是分布式实时大数据处理框架,是流计算中的佼佼者和主流，被业界称为实时版Hadoop。
>
> 和Storm相关的NoSQL数据库，Redis——基于内存的数据库。作用类似Spark Streaming。

```
实时计算Storm主要内容：
    1) Storm的基本概念
    2) Storm的应用场景
    3) Storm和Hadoop的对比 
    4) Storm集群的安装的linux环境准备 
    5) zookeeper集群搭建 
    6) Storm集群搭建
    7) Storm配置文件配置项讲解
    8) 集群搭建常见问题解决
    9) Storm常用组件和编程API：Topology、 Spout、Bolt
    10) Storm分组策略(stream groupings)
    11) 使用Strom开发一个WordCount例子
    12) Storm程序本地模式debug、Storm程序远程debug
    13) Storm事物处理
    14) Storm消息可靠性及容错原理
    15) Storm结合消息队列Kafka：消息队列基本概念(Producer、Consumer、Topic、Broker等)、消息队列Kafka使用场景、Storm结合Kafka编程API
    16) Storm Trident概念
    17) Trident state 原理
    18) Trident开发实例
    19) Storm DRPC(分布式远程调用)介绍
    20) Storm DRPC实战讲解
    21) Storm和Hadoop 2.x的整合：Storm on Yarn
```

```
内存数据库Redis主要内容：
    1) redis特点、与其他数据库的比较
    2) 如何安装redis
    3) 如何使用命令行客户端
    4) redis的字符串类型
    5) redis的散列类型
    6) redis的列表类型
    7) redis的集合类型
    8) 如何使用java访问redis【a.python访问redis,scala访问redis】
    9) redis的事务(transaction)
    10) redis的管道(pipeline)
    11) redis持久化(AOF+RDB)
    12) redis优化
    13) redis的主从复制
    14) redis的sentinel高可用
    15) twemproxy,codis实战
    16) redis3.x集群安装配置	
```

## 6.其他学习

### 6.1  Kafka

> Kafka是一个开源流处理平台——高吞吐量的分布式发布订阅消息系统，由Scala和Java编写。它可处理用户在网站中的所有动作流数据。类似的消息中间件还有RocketMQ。

```
消息纸中间件Kafka主要内容：
    1) kafka是什么
    2) kafka体系结构
    3) kafka配置详解
    4) kafka的安装
    5) kafka的存储策略
    6) kafka分区特点
    7) kafka的发布与订阅
    8) java编程操作kafka
    9) scala编程操作kafka
    10) flume 和kafka 的整合
    11) Kafka 和storm 的整合
```



### 6.2机器学习算法
```
1、python及numpy库
    1) 机器学习简介
    2) 机器学习与python
    3) python语言–快速入门
    4) python语言–数据类型详解
    5) python语言–流程控制语句
    6) python语言–函数使用
    7) python语言–模块和包
    8) phthon语言–面向对象
    9) python机器学习算法库–numpy
    10) 机器学习必备数学知识–概率论

2、常用算法实现
    1) knn分类算法–算法原理
    2) knn分类算法–代码实现
    3) knn分类算法–手写字识别案例
    4) lineage回归分类算法–算法原理
    5) lineage回归分类算法–算法实现及demo
    6) 朴素贝叶斯分类算法–算法原理
    7) 朴素贝叶斯分类算法–算法实现
    8) 朴素贝叶斯分类算法–垃圾邮件识别应用案例
    9) kmeans聚类算法–算法原理
    10) kmeans聚类算法–算法实现
    11) kmeans聚类算法–地理位置聚类应用
    12) 决策树分类算法–算法原理
    13) 决策树分类算法–算法实现
```



### 6.3、大型网站高并发处理

```
1) 第四层负载均衡
    a) Lvs负载均衡         i. 负载算法，NAT模式，直接路由模式（DR），隧道模式（TUN）
    b) F5负载均衡器介绍
2) 第七层负载均衡
    a) Nginx     b) Apache
3) Tomcat、JVM优化提高并发量
4) 缓存优化
    a) Java缓存框架       i. Oscache，ehcache
    b) 缓存数据库         i. Redis，Memcached
5) Lvs+nginx+tomcat+redis|memcache构建二层负载均衡千万并发处理
6) Haproxy
7) Fastdfs小文件独立存储管理
8) Redis缓存系统    a) Redis基本使用     b) Redis sentinel高可用     c) Redis好友推荐算法
```

### 6.4  Lucene基础

```
Lucene主要内容：
    1) Lucene介绍
    2) Lucene 倒排索引原理
    3) 建索引 IndexWriter
    4) 搜索 IndexSearcher
    5) Query
    6) Sort和 过滤 （filter）
    7) 索引优化和高亮
```

### 6.5  Solr基础

```
Solr主要内容：
    1) 什么是solr
    2) 为什么工程中要使用solr
    3) Solr的原理
    4) 如何在tomcat中运行solr
    5) 如何利用solr进行索引与搜索
    6) solr的各种查询
    7) solr的Filter
    8) solr的排序
    9) solr的高亮
    10) solr的某个域统计
    11) solr的范围统计
    12) solrcloud集群搭建
```

### 6.6  Federation

>  HDFS Federation即HDFS的联邦，可简单理解为多个HDFS集群聚合到一起，更准确的理解是有多个namenode节点的HDFS集群。

## 7.总结

> 常见大数据开源框架：
>
> 离线计算：Hadoop MapReduce、Spark
>
> 实时(流式)计算：Storm、Spark Streaming、S4、Heron
>
> 文件存储：Hadoop HDFS、Tachyon、KFS
>
> NoSQL数据库：HBase(列存储数据库)、Redis(K-V数据库)、MongoDB(文档数据库)、Neo4j(图形数据库)
>
> 资源管理：Yarn、Mesos
>
> 日志收集：Flume、Scribe、Logstash、Kibana
>
> 消息系统：Kafka、StormMQ、ZeroMQ、RabbitMQ
>
> 查询分析：Hive、Impala、Pig、Presto、Phoenix、Spark SQL、Drill、Flink、Kylin、Druid
>
> 分布式协调服务：ZooKeeper
>
> 集群管理与监控：Ambari、Ganglia、Nagios、Cloudera Manager
>
> 数据挖掘与机器学习：Mahout、Spark MLlib
>
> 数据同步：Sqoop、DataX
>
> 任务调度：Oozie
>
> 《写给大数据开发初学者的话》：https://www.cnblogs.com/benchao/p/7998248.html    https://zhuanlan.zhihu.com/p/26545566

![https://blog.csdn.net/qq_38872310/article/details/79758426](https://img-blog.csdn.net/20180330161832348)

## 8.其他方向

> 包括：JavaWeb、关系型数据库、前端三剑客(HTML、CSS、JavaScript)

## 9.项目案例

# 数据挖掘学习路线

**数据挖掘**，通常指一些由软件实现的机制，目的是从巨量数据中提取出信息。数据挖掘，含义广泛，往往又被称作算法。

> https://www.sohu.com/a/148723351_539390
>
> **数据挖掘 = 业务知识 +算法+工程能力**
>
> **算法 =自然语言处理技术（ NLP ） + 计算机视觉技术（ CV ） + 机器学习 / 深度学习（ ML/DL ）**
>
> **工程能力=C/Java/Python编程+分布式计算和数据处理**
>
> （ 1 ）其中业务知识具体指的是个性化推荐，计算广告，搜索，互联网金融等； NLP ， CV 分别是处理文本，图像视频数据的领域技术，可以理解为是将非结构化数据提取转换成结构化数据；最后的ml/dl 技术则是属于模型学习理论；
>
> （ 2 ）在选择岗位时，各个公司都没有一套标准的称呼，但是所做的事情无非 2 个大方向，一种是主要钻研某个领域的技术，比如自然语言处理工程师，计算机视觉工程师，机器学习工程师等；一种是将各种领域技术应用到业务场景中去解决业务需求，比如数据挖掘工程师，推荐系统工程师等；具体的称呼不重要，重要的是平时的工作内容；
>
> PS ：在互联网行业，数据挖掘相关技术应用比较成功的主要是推荐以及计算广告领域，而其中涉及到的数据主要也是文本，所以 NLP 技术相对来讲比较重要，至于 CV 技术主要还是在人工智能领域（无人车，人脸识别等）应用较多，本人了解有限，相关的描述会较少；
>
> \3. 根据之前的分析，也可以看到该岗位所需要的 3 种基本能力分别是业务经验，算法能力与工程能力；
>
> （二） 入门 1. 工程能力
>
> （ 1 ）编程基础：需要掌握一大一小两门语言，大的指 C++ 或者 Java ，小的指 Python 或者 shell 脚本；需要掌握基本的数据库语言；
>
> 建议： MySQL + python + C++ ；语言只是一种工具，看看语法就好；
>
> 推荐书籍：《 C++ primer plus 》
>
> （ 2 ）开发平台： Linux ；
>
> 建议：掌握常见的命令，掌握 Linux 下的源码编译原理；
>
> 推荐书籍：《 Linux 私房菜》
>
> （ 3 ）数据结构与算法分析基础：掌握常见的数据结构以及操作（线性表，队，列，字符串，树，图等），掌握常见的计算机算法（排序算法，查找算法，动态规划，递归等）；
>
> 建议：多敲代码，多上 OJ 平台刷题；
>
> 推荐书籍：《大话数据结构》《剑指 offer 》
>
> （ 4 ）海量数据处理平台： Hadoop （ mr 计算模型， java 开发）或者 Spark （ rdd 计算模型， scala开发），重点推荐后者；
>
> 建议：主要是会使用，有精力的话可以看看源码了解集群调度机制之类的；
>
> 推荐书籍：《大数据 spark 企业级实战》
>
> \2. 算法能力
>
> （ 1 ）数学基础：概率论，数理统计，线性代数，随机过程，最优化理论
>
> 建议：这些是必须要了解的，即使没法做到基础扎实，起码也要掌握每门学科的理论体系，涉及到相应知识点时通过查阅资料可以做到无障碍理解；
>
> （ 2 ）机器学习 / 深度学习：掌握 常见的机器学习模型（线性回归，逻辑回归， SVM ，感知机；决策树，随机森林， GBDT ， XGBoost ；贝叶斯， KNN ， K-means ， EM 等）；掌握常见的机器学习理论（过拟合问题，交叉验证问题，模型选择问题，模型融合问题等）；掌握常见的深度学习模型（ CNN ，RNN 等）；
>
> 建议：这里的掌握指的是能够熟悉推导公式并能知道模型的适用场景；
>
> 推荐书籍：《统计学习方法》《机器学习》《机器学习实战》《 UFLDL 》
>
> （ 3 ）自然语言处理：掌握常见的方法（ tf-idf ， word2vec ， LDA ）；
>
> \3. 业务经验
>
> （ 1 ）了解推荐以及计算广告相关知识；
>
> 推荐书籍：《推荐系统实践》《计算广告》
>
> （ 2 ）通过参加数据挖掘竞赛熟悉相关业务场景，常见的比赛有 Kaggle ，阿里天池， datacastle 等；
>
> PS: 以上都是一些入门级别的介绍，在长期的学习中，应该多看顶会 paper ，多读开源代码，多学习优秀解决方案；
>
> PS: 以上推荐的书籍对应的电子版可以联系本人
>
> \2. 海投
>
> （ 1 ）自找
>
> （ 2）一般投递简历时，尽量联系公司内部的师兄师姐或者熟人，帮忙将简历直接给到团队 leader 手中，尽量内推；
>
> （ 3 ）加入各种求职交流群，多认识些人，共享资源；
>
> \3. 面试
>
> （ 1 ）一般技术面有以下一些环节：自我介绍，项目介绍，算法提问（推公式），数据结构提问（写代码）；
>
> 1 ）自我介绍：一般尽量简短，主要讲清楚自己的研究方向，所取得成就以及优势所在即可；
>
> 2 ）项目介绍：简历上的项目一定要熟悉，介绍时候分三部曲：项目背景，项目方案，项目成果；对项目中涉及到的一些技术点一定要很熟悉；
>
> 3 ）算法提问：一般是问常见机器学习模型原理或者一些机器学习常见问题的解决方案（比如正负样本不平衡之类的），所以常见的机器学习模型一定要很清楚原理，必须会推公式，能知道工程实现的一些trick 的话，那你就离 sp 不远了；
>
> 4 ）数据结构提问：常见的数据结构一定要掌握，基础的代码一定要会手写（快排，冒泡，堆排，归并排序，二分查找，二叉树的遍历，二叉树增删查改）；剑指 offer 的题目要会；有精力的话可以刷下leetcode ；
>
> （ 2 ）面试的时候多准备一些素材，在面试过程中**主动引导**面试官提问。
>
> ​		比如面试官让你讲解 gbdt 原理时，这会你可以跟他说一般说起 gbdt ，我们都会跟 rf 以及 xgboost 一块讲，然后你就可以主动地向面试官输出你的知识；面试并不是死板地你问我答，而是一种沟通交流，所以尽可能地把面试转化成聊天式的对话，多输出自己一些有价值的观点而不是仅仅为了回答面试官的问题；
>
> （ 3 ）在面试过程中，除了基础的东西要掌握，可以适当地向面试官展示你的一些其他的亮点。
>
> ​		比如跟面试官谈论某些最近 paper 的进展以及一些技术方面的想法等，突出自己的与众不同；
>
> （ 4 ）常见面试题（由于有的面试时间较久，主要靠记忆写下来）
>
> 1 ）几种模型（ svm ， lr ， gbdt ， em ）的原理以及公式推导；
>
> 2 ） rf ， gbdt 的区别； gbdt ， xgboost 的区别（烂大街的问题最好从底层原理去分析回答）；
>
> 3 ）决策树处理连续值的方法；
>
> 4 ）特征选择的方法；
>
> 5 ）过拟合的解决方法；
>
> 6 ） kmeans 的原理，优缺点以及改进；
>
> 7 ）常见分类模型（ svm ，决策树，贝叶斯等）的优缺点，适用场景以及如何选型；
>
> 8 ） svm 为啥要引入拉格朗日的优化方法；
>
> 9 ）假设面试官什么都不懂，详细解释 CNN 的原理；
>
> 10 ）海量的 item 算文本相似度的优化方法；
>
> 11 ）梯度下降的优缺点；
>
> 12 ） em 与 kmeans 的关系；
>
> 13 ） L1 与 L2 的区别以及如何解决 L1 求导困难；
>
> 14 ）如何用尽可能少的样本训练模型同时又保证模型的性能；
>
> 15 ）解释 word2vec 的原理以及哈夫曼树的改进；
>
> 16 ）对推荐算法的未来看法；
>
> 17 ）在模型的训练迭代中，怎么评估效果；
>
> 18 ）有几个 G 的文本，每行记录了访问 ip 的 log ，如何快速统计 ip 出现次数最高的 10 个 ip ；如果只用 linux 指令又该怎么解决；
>
> 19 ）一个绳子烧完需要 1 个小时，假设所有绳子的材质都不一样，也不均匀，怎么取出 1 小时加 15 分钟；
>
> 20 ）假设有个 M*N 的方格，从最左下方开始往最右上方走，每次只能往右或者往上，问有多少种走法，假设中间有若干个格子不能走，又有多少种走法；
>
> 21 ）实现 hmm 的状态转移代码；
>
> 22 ）最短路径代码；
>
> 23 ）拼车软件是如何定价的以及如何优化；
>
> 24 ） 100 张牌，每次只能抽一张，抽过的牌会丢掉，怎么选出最大的牌；
>
> 25 ）怎么预测降雨量；
>
> 26 ） k-means 代码；
>
> 27 ） mr 方案解决矩阵相乘的代码；
>
> 28 ） sql 语句的一些优化技巧；
>
> 29 ）关于集群调度的一些经验 trick 掌握多少；
>
> 30 ）设计一个系统可以实时统计任意 ip 在过去一个小时的访问量；
>
> 31 ）设计 LRU 系统
>
> （ 5 ）不同公司的面试风格都略有不同：
>
> 1 ）百度：技术派，现场面，最大的风格就是写代码， 2 面技术加一面经理面，技术面必写代码；
>
> 2 ）阿里：内推可以电话面，主要是聊项目跟问一些基础的数据结构方面的知识，看看剑指 offer 一般可以应付；
>
> 3 ）腾讯：内推可以电话面，主要聊项目跟推公式；
>
> 4 ）华为：主要聊项目，智力题以及聊价值观之类的东西；
>
> 5 ）滴滴研究院：百度系，面试风格跟百度差不多；



```
https://blog.csdn.net/lzx704/article/details/79668178
数据挖掘基础
数据挖掘的概念：
从数据中“淘金”，从大量数据（文本）中挖掘出隐含的、未知的、对决策有潜在的关系、模型和趋势，并用这些知识和规则建立用于决策支持的模型，提供预测性决策支持的方法、工具和过程，这就是数据挖掘。
它是利用各种分析工具在大量数据中寻找其规律和发现模型与数据之间关系的过程，是统计学、数据技术和人工职能技术的综合。

数据挖掘的基本任务：
包括利用分类与预测、聚类分析、关联规则、时序模式、偏差检测、职能推荐等方法，帮助企业提取数据中蕴含的商业价值，提高企业的竞争力。

数据挖掘建模过程
![屏幕快照 2018-03-18 上午9.58.40](http://ovblf5i76.bkt.clouddn.com/2018-03-18-屏幕快照 2018-03-18 上午9.58.40.png)

1 目标定义
任务理解
指标确认
针对具体的挖掘应用需求
明确本次挖掘目标是什么？
系统完成后能达到什么样的效果？

2 数据采集
建模抽样
抽样数据的标准，一是相关性、二是可靠性、三是有效性。

抽样的方式
随机抽样：比如按10%比例随机抽样
等距抽样：比如按5%比例，一共100组，取20、40、60、80、100
分层抽样：将样本分若干层次，每个层次设定不同的概率。
从起始顺序抽样：从输入数据集的起始处开始。
分类抽样：依据某种属性的取值来选择数据子集。如按客户名称分类、按地址区域分类等。分类抽样的选取方式就是前面所述的几种方式，只是抽样以类为单位。

质量把控

实时采集

3 数据整理
数据探索
对所抽样的样本数据进行探索、审核和必要的加工处理，是保证最终的挖掘模型的质量所必须的。
常用的数据探索方法主要包括两方面：数据质量分析，数据特征分析。

数据质量分析：得主要任务是检查原始数据中是否存在脏数据。包括缺失值分析、异常值分析、数据一致性分析。
数据特征分析：在质量分析后可通过绘制图标、计算某种特征量等手段进行特征分析，
主要包括
分布分析：能揭示数据的分布特征和分布类型。可用直方图、饼图、条形图等展示
对比分析：将两个相互联系的指标进行比较，从数据量上展示和说明研究对象规模的大小，水平的高低，速度的快慢，以及各种关系是否协调。比如，各部门的销售金额的比较、各年度的销售额对比。
统计量分析：用统计指标对定量数据进行统计描述，常从集中和离中趋势两个方面进行分析。平均水平的指标是对个体集中趋势的度量，最广泛是均值和中位数；反映变异程度的指标则是对个体离开平均水平的度量，使用较广泛的是标准差（方差）、四分卫间距。

周期性分析：分析某个变量是否跟着时间变化而呈现出某种周期变化趋势。
贡献度分析：原理是帕累托法则（又称20/80定律）
相关性分析：分析连续变量之间线性相关程度的强弱，并用适当的统计指标表示出来的过程称为相关分析。判断两个变量是否具有线性相关关系的最直观的方法是直接绘制散点图。多元线性回归。

数据清洗
数据清洗主要是删除原始数据集中的无关数据、重复数据、平滑噪音数据，刷选调与挖掘主题无关的数据，处理缺失值，异常值等。
缺失值处理:删除记录、数据插补和不处理。
异常值处理：直接删除、提油现有变量，进行填补。

数据变换
数据变换主要是对数据进行规范化处理，将数据转换成“适当”形势，以适用与挖掘任务与算法的需要。
常见的数据变换方法，简单函数变换、规范化、连续属性离散化，属性构造，小波变换。

数据规约
数据规约产生更小但保持元数据完整性的新数据集。提高效率。主要包括属性规约和数值规约。

数据集成
数据来源往往分布在不同的数据源中，数据集成就是将数据源合并存在一个一致性的数据存储。

4 构建模型
样本抽取完并经预处理，对本次建模进行确认，是分类、聚合、关联规则、时序模式或者职能推荐，以便后续选用哪种算法进行模型构建。这一步是核心环节。
针对餐饮行业的数据挖掘应用，挖掘建模主要基于关联规则算法的动态菜品职能推荐、基于聚类算法的餐饮客户价值分析、基于分类与预测算法的菜品销售预测、基于整体优化的新店选址。

模型发现
构建模型
验证模型
5 模型评价
为了确保模型有效，需要对其进行测试评价，目的找出一个最好的模型。
为了有效判断一个越策模型的性能表现，需要一组没有参与预测模型建立的数据集，并在该数据集上评价预测模型的精准率。

设定评价标准
多模型对比
模型优化
6 模型发布
模型部署
模型重构
小结
本章从一个知名餐饮企业经营过程中存在的困惑出发，引出数据挖掘的概念、基本任务、建模过程。
针对建模过程，简要分析了定义挖掘目标、数据取样、数据塔索、数据预处理以及挖掘建模的各个算法概述和模型评价。
如何帮助企业从数据中洞察商机、提取价值，这是现阶段几乎所有企业都关心的问题。通过发生在身边的案例，由浅入深引出深奥的数据挖掘理论，让读者感悟数据挖掘的非凡魅力。点赞
个人看完这一章，对于数据挖掘的落地有了一个大概得了解，我们选择、使用、学习这些大数据的技术应该是结果导向的，这里会让人更清晰去选择技术，使用技术。
```



## Hadoop基础

大数据技术，是指从各种类型的数据中，快速获得有价值信息的能力。适用大技术的技术，包括大规模并行处理（MPP）数据库、数据挖掘、分布式文件系统、分布式数据库、云计算平台、互联网和可扩展的存储系统。

大数据特点4V

- 数据量大（Volume）
- 数据类型复杂（Variety）
- 数据处理速度快（Velocity）
- 数据真实性高（Veracity）

当前，Hadoop已经成为了事实上的标准。Hadoop除了社区版，还有其他厂商发行的版本。

- Cloudera：最成型的发行版本，拥有最多的部署案例；
- Hortonworks：100%开源的Apache Hadoop唯一提供商。
- MapR：
- Amazon Elastic Map Reduce(EMR)：这是一个托管的解决方案。

### 生态系统

Hadooop生态系统主要包括:Hive、HBase、Pig、Sqoop、Flume、Zookeeper、Mahout、Spark、Storm、Shark、Phoenix、Tex、Ambari

![img](http://ovblf5i76.bkt.clouddn.com/2018-03-23-15217909659823.jpg)

#### Hive[haɪv]:数据仓库系统

用于Hadoop的一个数据仓库系统，它提供了类似SQL的查询语言，通过使用该语言， 可以方便地进行数据汇总，特定查询以及分析存放在Hadoop兼容文件系统中的大数据。

hive基于hdfs构建了数据仓库系统，它以hdfs作为存储，依赖于数据库(嵌入式的数据库derby或者独立的数据mysql或oracle)存储表schema信息，并完成基于sql自动解析创建mapreduce任务(由于mapreduce计算效率比较差，目前官方推荐的是底层计算模型采用tez或者spark)。
所以hive可以理解为：hdfs原始存储+DB Schema信息存储+SQL解析引擎+底层计算框架组成的数据仓库。

#### Hbase:分布式数据库

一种分布式、可伸缩的、大数据库存储库，支持随机、实施读/写访问。

#### Pig:工作流引擎

Pig是一种编程语言，它简化了Hadoop常见的工作任务。Pig可加载数据、表达转换数据以及存储最终结果。Pig内置的操作使得半结构化数据变得有意义（如日志文件）。同时Pig可扩展使用Java中添加的自定义数据类型并支持数据转换。

#### sqoop[skup]:数据库ETL工具

为高效传输批量数据而设计的一种工具，其用于Apache Hadoop和结构化数据存储库如关系数据库 之间的数据传输。

#### Flume:日志收集

一种分布式、可靠的、可用的服务，其用于高效搜集、汇总、移动大量日志数据

#### ZooKeeper[ˈzu:ki:pə(r)]：协同服务管理

一种集中服务、其用于维护配置信息，命名，提供分布式同步，以及提供分组服务。

#### HDFS:分布式数据存储系统

hdfs是大数据系统的基础，它提供了基本的存储功能，由于底层数据的分布式存储，上层任务也可以利用数据的本地性进行分布式计算。hdfs思想上很简单，就是namenode负责数据存储位置的记录，datanode负责数据的存储。使用者client会先访问namenode询问数据存在哪，然后去datanode存储；写流程也基本类似，会先在namenode上询问写到哪，然后把数据存储到对应的datanode上。所以namenode作为整个系统的灵魂，一旦它挂掉了，整个系统也就无法使用了。在运维中，针对namenode的高可用变得十分关键。

#### Mahout[məˈhaʊt]:算法集

一种基于Hadoop的机器学习和数据挖掘的分布式计算框架算法集，实现了多重MapReduce模式的数据挖掘算法。

#### spark:计算模型

一种开源的数据分析集群计算框架，建立于HDFS之上。和Hadoop一样，用于构建大规模、低延时的数据分析应用。它采用Scala语言实现，使用Scala作为应用框架。

spark是现在大数据中应用最多的计算模型，它与java8的stream编程有相同的风格。封装了很多的计算方法和模型，以延迟执行的方式，在真正需要执行的时候才进行运算。既可以有效的做计算过程的容错，也可以改善我们的编程模型。
Spark是一款很棒的执行引擎，我们可以看到大部分的Spark应用，是作为Hadoop分布式文件系统HDFS的上层应用。
*（ Spark 典型的取代了已经过时的MapReduce引擎，与Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）或者分布式计算框架Mesos一起工作，有时候同时与两者一起作为一个计划进行）*
但是Cutting强调：“**还有许多事情Spark是做不到的。”比如：它不是一个全文本搜索引擎；是Solr在Hadoop里扮演着这个角色**。它可以运行SQL查询对抗Spark，但是它没有被设计成一个交互式查询系统，对此，Cutting提出，Impala可以实现交互查询。

如果你只是要需要进行streaming 编程或者batch 编程，那么你需要一个执行引擎，Spark就是很棒的一个。但是人们想做的事情远不止于此，他们想实现交互式SQL（结构化查询语言），他们想实现搜索，他们想做各种涉及系统的实时处理，如Kafka(一种高吞吐量的分布式发布订阅消息系统)…我认为那些认为Spark就是整个堆的人是确实存在的少数情况。

#### Storm:

一个分布式、容错的实时计算系统。

#### Shark[ʃɑ:k]:SQL查询引擎

Hive on Spark，一个专门为Spark打造的大规模数据仓库系统，兼容Apache Hive。无需修改现有的数据或者查询，就可以用100倍的速度执行Hive SQL。Shark支持Hive查询语言、元存储、序列化格式及自定义函数，与现有Hive部署无缝集成，是一个更快、更强大的替代方案。

#### Phoenix:

一个构建在Apache HBase之上的一个SQL中间层，完全使用Java编写，提供了一个客户端可嵌入的JDBC驱动。

#### Tez:

一个机遇Hadoop YARN之上的DAG计算框架。它把Map/Reduce过程拆分成若干个子过程。同时可以把多个Map/Reduce任务组合成一个较大的DAG任务，减少Map/Reduce之间的文件存储。同时合理组合其子过程，减少任务的运行时间。

#### Amari:安装部署工具

一个供应、管理和监视Apache Hadoop集群的开源框架，它提供一个直观的操作工具和一个健壮的Hadoop Api,

#### MapReduce:

说穿了就是函数式编程，把所有的操作都分成两类，map与reduce，map用来将数据分成多份，分开处理，reduce将处理后的结果进行归并，得到最终的结果。

#### ChuKwa:

#### YARN[jɑ:n]：Hadoop 资源管理器

### Hadoop HDFS

HDFS被设计成适合在通用硬件上的分布式文件系统。具有如下特点

- 具有高度容错性的系统。设计用来部署在低廉的硬件上，提供高吞吐量，适合那些有超大数据集的应用程序，放宽了POSIX的要求这样可以实现以**流的形式（streaming access）**访问文件系统中的数据。
- HDFS采用master/slave。一个集群由一个NameNode和多个DataNodes组成。

![img](http://images2017.cnblogs.com/blog/789140/201709/789140-20170926023531870-1237924956.jpg)

- Active Namenode：主 Master（只有一个），管理 HDFS 的名称空间，管理数据块映射信息；配置副本策略；处理客户端读写请求。
- Secondary NameNode：NameNode 的热备；定期合并 fsimage 和 fsedits，推送给 NameNode；当 Active NameNode 出现故障时，快速切换为新的 Active NameNode。
- Datanode：Slave（有多个）；存储实际的数据块；执行数据块读 / 写。
- Client：与 NameNode 交互，获取文件位置信息；与 DataNode 交互，读取或者写入数据；管理 HDFS、访问 HDFS。

## Hive

### 概念

Hive最初是Facebook面对海量数据和机器学习的需求而产生和发展的，是建立在Hadoop上数据仓库基础架构，它可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能。
Hive作为数据仓库，提供一系列工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。
Hive定义了简单的类SQL查询语言，成为HQL，它允许熟悉SQL用户查询数据。

### 特点

- 支持索引，加快数据查询。
- 不同的存储类型，如纯文本文件、HBase中的文件。
- 将元数据保存在关系数据库中，大大减少了在查询过程中执行语义检查的时候。
- 可以直接使用存储在Hadoop文件系统中的数据。
- 内置大量用户函数UDF来操作时间、字符串和其他的数据挖掘工具，支持用户扩展UDF函数来完成内置函数无法实现的操作。
- 类SQL的查询方式，将SQL查询转换为MapReduce的Job在Hadoop集群上执行

Hive并不能够在大规模数据集上实现低延迟快速的查询，不能提供实时的查询和基于行级的数据更新操作。比如几百MB的数据集上执行查询一般有分钟级的时间延迟。所以它不适合低延迟的应用。最佳应用在大数据集的批处理作业，如网络日志分析。

### Hive支持的数据模型

表：存在在HDFS目录底下，固定目录
外部表：跟表差不多，指定目录
分区：
桶：对指定的列计算其哈希值，根绝哈希值切分数据，目的是并行，每个桶对应一个文件。

## Hbase

[Hbase的应用场景、原理及架构分析](http://blog.csdn.net/xiangxizhishi/article/details/75388971)
[深入了解HBASE架构（转）](https://www.cnblogs.com/ajianbeyourself/p/7790044.html#_label0)
[B-，B树，B+](https://www.cnblogs.com/wjoyxt/p/5501706.html)

### 概念

Hbase是一个分布式、**面向列**的开源数据库，利用HBASE技术可以在廉价PC服务器搭建大规模结构化存储集群。它不是关系型数据库，是一个适合非结构化的数据存储数据库。它利用Hadoop MapReduce来处理HBase中的海量数据，同时利用Zookeeper作为其协同服务。
采购LSM算法，后面继续深入研究，这个算法，实在内存中对未排序的值进行，拆分排序，比如N个数，每M个拆分一次做排序，那么每次寻找的计算量应该是N/M*log2M

### 特点

- 线性和模块化可扩展性
- 严格一致的读取和写入
- 表的自动配置和分片
- 支持RegionServers之间的自动故障转移
- 方便的基类支持Hadoop的MapReduce作业与Apache HBase的表
- 易于使用的Java API的客户端访问
- 块缓存和布鲁姆过滤器实时查询
- Thrift网管和REST-FUL Web服务支持XML、protobuf和二进制的数据编码选项；
- 可扩展的基于JRuby（JIRB）的脚本；
- 支持监控信息通过Hadoop子系统导出到文件或Ganglia

Pig和Hive还未HBase提供了高层语言支持，这使得HBase上进行数据统计处理变得非常简单。Sqoop则为HBase提供了RDBMS数据导入功能，使用传统数据库向HBase迁移变得很方面。

### 原理

![img](https://img-blog.csdn.net/20160706195030446?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
HBase构建在HDFS之上，其组件包括 Client、zookeeper、HDFS、Hmaster以及HRegionServer。Client包含访问HBase的接口，并维护cache来加快对HBase的访问。Zookeeper用来保证任何时候，集群中只有一个master，存贮所有Region的寻址入口以及实时监控Region server的上线和下线信息。并实时通知给Master存储HBase的schema和table元数据。HMaster负责为Region server分配region和Region server的负载均衡。如果发现失效的Region server并重新分配其上的region。同时，管理用户对table的增删改查操作。Region Server 负责维护region，处理对这些region的IO请求并且切分在运行过程中变得过大的region。

Hbase底层使用还是Hadoop的HDFS。同时包含3个重要组件，
**Zookeeper：**为整个HBase集群提供协助的服务（信息传输）；
**HMaster：**监控和操作集群中所有的RegionServer；
**HregionServer：**服务和管理分区（regions）。

**Region：**Hbase的Table中的所有行都按照row key的字典序排列。Table 在行的方向上分割为多个Region。、Region按大小分割的，每个表开始只有一个region，随 着数据增多，region不断增大，当增大到一个阀值的时候， region就会等分会两个新的region，之后会有越来越多的 region。
![img](https://img-blog.csdn.net/20160706194306112?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
![img](https://img-blog.csdn.net/20160706194547894?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
Region是HBase中分布式存储和负载均衡的最小单元。 不同Region分布到不同RegionServer上。
![img](https://img-blog.csdn.net/20160706194713159?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
Region虽然是分布式存储的最小单元，但并不是存储 的最小单元。Region由一个或者多个Store组成，每个store保存一个 columns family。每个Strore又由一个memStore和0至多个StoreFile组成。memStore存储在内存中，StoreFile存储在HDFS上。
![img](https://img-blog.csdn.net/20160706194845926?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

### HBase和RDBMS的区别

HBASE设计的初衷是针对大数据进行随机地、实时地读写操作。区别

| 特点类型 | HBase                                                        | RDBMS                                                        |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据类型 | 简单的字符串类型，用户自己处理                               | 有数值、字符串、时间类型等丰富选择                           |
| 数据操作 | 简单的插入、查询、删除、清空等操作，表和表分离               | 有各种各样的函数、连接操作等。                               |
| 存储模式 | 基于列存储，每个列由几个文件保存，不同列的文件是分离的       | 基于表结构和行模式保存                                       |
| 数据维护 | 更新操作不应该叫更新，实际是插入数据                         | 真正替换                                                     |
| 可伸缩性 | 方便、快速                                                   | 需要增加中间层，复杂                                         |
| 具体应用 | 适应海量存储和互联网应用，灵活分布式架构可以组建大的数据仓库 | 具有ACID特性，丰富的SQL，具备面向磁盘存储、带有索引结构、多线程访问、基于锁的同步访问机制、基于log记录的恢复机制等 |

### HBase数据模型

传统型数据库以行的形式存储数据，每行数据包含多列，每列只有单个值。在HBase中，数据实际存储在一个“映射”中，并且“映射”的键（key）是被排序的。类似JavaScript Object(JSON)
HBase包含如下几个概念：
1 Row key
一条记录的唯一标示
2 column family
一列数据的集合的存储体，作为列簇
3 Column qualifier
在列簇中的每个列数据的限定符，用于指定数据的属性
4 Cell
实际存储的数据，包含数据和时间戳
![img](https://img-blog.csdn.net/20160706193846037?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)

### 小结

这里介绍大数据数据库HBASE的基础概念,分析了HBase的原理，主要包括其与RDBMS的对比、访问接口、数据模型等。最后结构HBase的架构图介绍各个模块组件，包括HMaster、HRegionServer、Zookeeper

## 大数据挖掘建模平台

本章首先介绍常用的大数据平台，采用开源的TipDM-HB大数据挖掘建模平台。
SOA架构，面向服务架构，以为着服务接口、流程整合、资源可利用、管控。

## 挖掘建模

经过数据探索与数据预处理，得到了可以建模的数据。
根据挖掘目标和数据形式可以建立分类与预测、聚类分析、关联规则、职能推荐等模型。





















































